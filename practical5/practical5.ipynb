{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'nngraph'\n",
    "require 'nn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  4\n",
       "  6\n",
       " 13\n",
       "[torch.DoubleTensor of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Try out nn.CAddTable which does element-wise addition\n",
    "\n",
    "add = nn.CAddTable()\n",
    "t1 = torch.Tensor({1,2,3})\n",
    "t2 = torch.Tensor({3,4,10})\n",
    "print(add:forward({t1, t2}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Regarding the gradients of CAddTable\n",
    "-- For this layer, we only have a gradient with respect to the input. There are no parameters of this layer.\n",
    "-- Since we are just adding, dLoss/dinput_i = dLoss/dOutput for all of the input tensors of this layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 60\n",
       " 30\n",
       " 50\n",
       "[torch.DoubleTensor of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "--nngraph overloads the parenthesis, \"()\", operator on all nn.Module objects\n",
    "-- So we can rewrite the previous addition example using this notation\n",
    "\n",
    "x1 = nn.Identity()() -- first () specify no arguments. second () specify input to module. Blank input indicates that it is a \"dummy input\", ie input of entire network\n",
    "x2 = nn.Identity()()\n",
    "a = nn.CAddTable()({x1, x2})\n",
    "m = nn.gModule({x1, x2}, {a})\n",
    "\n",
    "-- Note. nn.Identity() is an object of class nn.Module. But nn.Identity()() is a graph node\n",
    "-- nn.gModule wraps the graph into a single object, which we can use as a normal nn.Module()\n",
    "\n",
    "t1 = torch.Tensor({10, 20, 30})\n",
    "t2 = torch.Tensor({50, 10, 20})\n",
    "print(m:forward({t1, t2}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear Weights\t\n",
       " 1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1\n",
       "[torch.DoubleTensor of size 5x10]\n",
       "\n",
       "Output of network\t\n",
       " 226\n",
       " 230\n",
       " 234\n",
       " 238\n",
       " 242\n",
       "[torch.DoubleTensor of size 5]\n",
       "\n",
       "\n",
       "Output of linear layer\t\n",
       " 56\n",
       " 57\n",
       " 58\n",
       " 59\n",
       " 60\n",
       "[torch.DoubleTensor of size 5]\n",
       "\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in1 = torch.ones(5) * 2\n",
    "in2 = torch.ones(5) * 4\n",
    "in3 = torch.range(1,10)\n",
    "\n",
    "x1 = nn.Identity()()\n",
    "x2 = nn.Identity()()\n",
    "x3 = nn.Identity()()\n",
    "\n",
    "linear = nn.Linear(10, 5)({x3})\n",
    "weights = torch.ones( linear.data.module.weight:size() ) -- initialise the linear weights to 1 so they are easy to debug\n",
    "linear.data.module.weight = weights\n",
    "linear.data.module.bias = torch.range(1,5)\n",
    "\n",
    "print('Linear Weights')\n",
    "print(linear.data.module.weight)\n",
    "\n",
    "c1 = nn.CMulTable()({x2, linear})\n",
    "z = nn.CAddTable()({x1, c1})\n",
    "\n",
    "net = nn.gModule({x1, x2, x3}, {z})\n",
    "\n",
    "print(\"Output of network\")\n",
    "print(net:forward({in1, in2, in3}))\n",
    "print(\"\\nOutput of linear layer\")\n",
    "print(linear.data.module.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
